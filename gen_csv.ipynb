{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 sample(s) successfully\n",
      "Processed 0 sample(s) with errors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from shutil import copy\n",
    "import pymatgen as mg\n",
    "import string_misc\n",
    "\n",
    "datadir = os.path.join(os.environ['USERPROFILE'],'OneDrive - Colorado School of Mines/Research/MIDDMI/TCO/Data')\n",
    "indir = os.path.join(datadir,'in') #libraries to be processed\n",
    "outdir = os.path.join(datadir,'out') #output directory for csvs and logs\n",
    "procdir = os.path.join(datadir,'processed') #destination dir for successfully processed libraries\n",
    "errdir = os.path.join(datadir,'error') #destination dir for libraries with errors\n",
    "config = os.path.join(datadir,'config') #config/parameter file location\n",
    "\n",
    "############################\n",
    "\"read config files\"\n",
    "############################\n",
    "os.chdir(config)\n",
    "\n",
    "#define A and B site atoms\n",
    "Asite = ['Ba']\n",
    "Bsite = ['Co','Fe','Zr','Y']\n",
    "\n",
    "#load variable definitions\n",
    "#----------------------------\n",
    "vardictdf = pd.read_excel('VariableDict.xlsx')\n",
    "\n",
    "#put standard variables into dict\n",
    "stdvardf = vardictdf[vardictdf.Keyword.str[0] != '!']\n",
    "stdvars = list(stdvardf.VarName)\n",
    "headers = list(stdvardf.Keyword + ': ' + stdvardf.DisplayName)\n",
    "stdvardict = dict(zip(stdvars,headers))\n",
    "\n",
    "#composition variables\n",
    "compvardf = vardictdf[vardictdf.Keyword == '!composition']\n",
    "compvars = list(compvardf.VarName)\n",
    "\n",
    "#load PLD parameters\n",
    "#----------------------------\n",
    "prepstep = 'Pulsed Laser Deposition'\n",
    "params = pd.read_excel('PLD_params.xlsx',skiprows=3,index_col=0)\n",
    "paramdictdf = pd.read_excel('PLD_ParamDict.xlsx')\n",
    "#put standard params into dict\n",
    "stdparamdf = paramdictdf[paramdictdf.Keyword.str[0] != '!']\n",
    "stdparams = list(stdparamdf.ParamName)\n",
    "parheaders = list(stdparamdf.Keyword + ': ' + stdparamdf.DisplayName)\n",
    "stdparamdict = dict(zip(stdparams,parheaders))\n",
    "\n",
    "#get lists of targets & target info params\n",
    "tgtcols = list(paramdictdf[paramdictdf.Keyword == '!target'].ParamName)\n",
    "tgtinfo = paramdictdf[paramdictdf.Keyword == '!targetinfo']\n",
    "tgtvars = paramdictdf[paramdictdf.Keyword=='!targetinfo'].DisplayName.unique()\n",
    "\n",
    "#rename headers to ingestable format using paramdict\n",
    "params = params.rename(index=str, columns = stdparamdict) \n",
    "params.insert(1,'PREPARATION STEP NAME',prepstep)\n",
    "\n",
    "#load list of possible targets\n",
    "targets = pd.read_excel('targetlist.xlsx')\n",
    "\n",
    "#library prefix to strip out\n",
    "libprefix = 'PDAC_COM3_'\n",
    "\n",
    "############\n",
    "\"end config\"\n",
    "############\n",
    "\n",
    "\n",
    "########################################\n",
    "\"loop through all libraries in indir\"\n",
    "########################################\n",
    "os.chdir(indir)\n",
    "nsucc = 0\n",
    "nerr = 0\n",
    "errsumname = os.path.join(errdir, 'gen_csv_' + datetime.now().strftime('%Y%m%d_%H%M%S') + '.err')\n",
    "errsumtxt = ''\n",
    "\n",
    "for lib in next(os.walk(indir))[1]:\n",
    "    os.chdir(lib)\n",
    "    \n",
    "    #log file\n",
    "    logname = lib + '_gen_csv.log'\n",
    "    log = open(logname, 'w')\n",
    "    log.write('Processing library ' + lib + '\\n')\n",
    "    \n",
    "    #error file\n",
    "    errtxt = ''\n",
    "    \n",
    "    #csv\n",
    "    csvname = lib + '_AllVar.csv'\n",
    "    \n",
    "    all_var = pd.DataFrame()\n",
    "    \n",
    "    #read the Points file to determine file length\n",
    "    try:\n",
    "        pts = pd.read_csv(lib + '_Points.txt',sep='\\t',usecols = [0]) \n",
    "    except FileNotFoundError: #starting with 1502 points filename suffix changed to '_Point'\n",
    "        pts = pd.read_csv(lib + '_Point.txt',sep='\\t',usecols = [0]) \n",
    "    \n",
    "    #dataframes to store composition data\n",
    "    chm = pd.DataFrame()\n",
    "    sitesum = pd.DataFrame(np.zeros((len(pts),2)), columns = ['A','B'])\n",
    "    \n",
    "    #track found variables\n",
    "    foundvars = ['Point','Row','Column']\n",
    "    \n",
    "    ################################\n",
    "    \"read variables from txt files\"\n",
    "    ################################\n",
    "    for fname in glob.glob(lib + '*.txt'):\n",
    "        vname = fname[len(lib)+1:fname.find('.txt')]\n",
    "        #regular variables\n",
    "        if vname in stdvars and vname not in foundvars:\n",
    "            if len(all_var) == 0:\n",
    "                cols = None\n",
    "            else:\n",
    "                cols = [3]\n",
    "            df = pd.read_csv(fname,sep='\\t',usecols = cols)\n",
    "            all_var = pd.concat([all_var,df], axis=1)\n",
    "        #composition variables\n",
    "        elif vname in compvars:\n",
    "            dfc = pd.read_csv(fname,sep='\\t',usecols = [3])\n",
    "            elmnt = vname[:vname.find('_at')]\n",
    "            chm[elmnt] = dfc[vname] \n",
    "            sitesum.B = sitesum.B + dfc[vname]\n",
    "        \n",
    "        foundvars.append(vname)\n",
    "    \n",
    "    #identify ignored and missing variables\n",
    "    ignoredvars = np.setdiff1d(foundvars,stdvars+compvars, assume_unique=True)\n",
    "    missingstd = np.setdiff1d(stdvars,foundvars, assume_unique=True)\n",
    "    missingcomp = np.setdiff1d(compvars,foundvars, assume_unique=True)\n",
    "    if len(ignoredvars) > 0:\n",
    "        log.write('Ignored variables:\\n\\t' + '\\n\\t'.join(ignoredvars) + '\\n')\n",
    "    if len(missingstd) > 0:\n",
    "        log.write('*Warning: missing standard variables:\\n\\t' + '\\n\\t'.join(missingstd) + '\\n')\n",
    "    if len(missingcomp) > 0:\n",
    "        log.write('*Warning: missing composition variables:\\n\\t' + '\\n\\t'.join(missingcomp) + '\\n')\n",
    "        errtxt += 'Missing composition variables: ' + ', '.join(missingcomp) + '\\n'\n",
    "        \n",
    "    #######################################\n",
    "    \"determine composition and add features\"\n",
    "    #######################################\n",
    "    #determine scale - starting at 1502 scale changes to 100\n",
    "    if sitesum.B.max() > 1:\n",
    "        sitesum.A = 100 - sitesum.B\n",
    "    else:\n",
    "        sitesum.A = 1 - sitesum.B\n",
    "    sitemax = sitesum.max(axis=1)\n",
    "    #normalize for the higher occupancy site\n",
    "    sitenorm = sitesum.divide(sitemax,axis=0) \n",
    "    chmnorm = chm.divide(sitemax,axis=0)\n",
    "    chmnorm.insert(0,'Ba',list(sitenorm.A))\n",
    "    chmnorm = chmnorm.round(5)\n",
    "    \n",
    "    \n",
    "    chmfeat = pd.DataFrame(np.zeros((44,13)),\n",
    "                           columns=['A_avg_X',\n",
    "                                'A_avg_at_radius',\n",
    "                                'A_avg_ion_radius',\n",
    "                                'A_avg_mass',\n",
    "                                'B_avg_X',\n",
    "                                'B_avg_at_radius',\n",
    "                                'B_avg_ion_radius',\n",
    "                                'B_avg_mass',\n",
    "                                'AB_site_ratio',\n",
    "                                'AB_X_ratio',\n",
    "                                'AB_at_radius_ratio',\n",
    "                                'AB_ion_radius_ratio',\n",
    "                                'AB_mass_ratio']\n",
    "                      )\n",
    "    \n",
    "    for elmnt in list(chmnorm.columns):\n",
    "        x = mg.Element(elmnt)\n",
    "        if elmnt in Asite:\n",
    "            wt = chmnorm[elmnt]/sitenorm.A #weight for wtd average\n",
    "            chmfeat.A_avg_X += wt*x.X\n",
    "            chmfeat.A_avg_at_radius += wt*x.atomic_radius\n",
    "            chmfeat.A_avg_ion_radius += wt*x.average_ionic_radius\n",
    "            chmfeat.A_avg_mass += wt*x.atomic_mass\n",
    "        elif elmnt in Bsite:\n",
    "            wt = chmnorm[elmnt]/sitenorm.B #weight for wtd average\n",
    "            chmfeat.B_avg_X += wt*x.X\n",
    "            chmfeat.B_avg_at_radius += wt*x.atomic_radius\n",
    "            chmfeat.B_avg_ion_radius += wt*x.average_ionic_radius\n",
    "            chmfeat.B_avg_mass += wt*x.atomic_mass\n",
    "        \n",
    "        chmnorm[elmnt] = elmnt + chmnorm[elmnt].map(str)\n",
    "            \n",
    "    chmnorm['formula'] = chmnorm.apply(lambda x: ''.join(x),axis=1)\n",
    "    chmnorm['formula'] = chmnorm['formula'] + 'O3'\n",
    "    \n",
    "    chmfeat.AB_site_ratio = sitenorm.A/sitenorm.B\n",
    "    chmfeat.AB_X_ratio = chmfeat.A_avg_X/chmfeat.B_avg_X\n",
    "    chmfeat.AB_at_radius_ratio = chmfeat.A_avg_at_radius/chmfeat.B_avg_at_radius\n",
    "    chmfeat.AB_ion_radius_ratio = chmfeat.A_avg_ion_radius/chmfeat.B_avg_ion_radius\n",
    "    chmfeat.AB_mass_ratio = chmfeat.A_avg_mass/chmfeat.B_avg_mass\n",
    "    for col in chmfeat.columns:\n",
    "        chmfeat = chmfeat.rename(columns={col:'PROPERTY: ' + col})\n",
    "        \n",
    "    sample = lib[len(libprefix):]\n",
    "\n",
    "    all_var = all_var.rename(columns = stdvardict) #rename headers to ingestable format using vardict\n",
    "    all_var.insert(0,'IDENTIFIER: Sample number',sample) #add sample column        \n",
    "    all_var.insert(0,'FORMULA',list(chmnorm['formula'])) #add formula column\n",
    "    all_var = pd.concat([all_var,chmfeat],axis=1)\n",
    "    \n",
    "    ####################\n",
    "    \"add PLD parameters\"\n",
    "    ####################\n",
    "    if sample in list(params.index) and not(pd.isnull(params.loc[sample,'IDENTIFIER: Date'])):\n",
    "\n",
    "        sp = params.loc[sample,:]\n",
    "        sp = sp.to_frame().T #convert series to df\n",
    "\n",
    "        #add columns for all possible targets (BZY82, BZC19, ...)\n",
    "        for var in tgtvars:\n",
    "            for tgt in targets.Target: \n",
    "                colname = 'PREPARATION STEP DETAIL: ' + tgt + ' ' + var\n",
    "                #insert 0 for all target vars\n",
    "                sp.insert(len(sp.columns),colname,0)\n",
    "            if var == 'Pulses/Cycle':\n",
    "                sp.insert(len(sp.columns),'PREPARATION STEP DETAIL: Average Target Pulses/Cycle',0)\n",
    "        \n",
    "        #get avg pulses per cycle across targets\n",
    "        ppc = []\n",
    "        for col in tgtinfo[tgtinfo.DisplayName=='Pulses/Cycle'].ParamName:\n",
    "            val = sp.loc[sample,col]\n",
    "            if str(val).find('/') >= 0: #if multiple values for single target, take average (should only apply to BZY82)\n",
    "                vals = [float(v) for v in val.split('/')]\n",
    "                val = np.mean(vals)\n",
    "            if string_misc.is_number(str(val)):\n",
    "                ppc.append(val)\n",
    "        sp.loc[sample,'PREPARATION STEP DETAIL: Average Target Pulses/Cycle'] = np.mean(ppc)\n",
    "\n",
    "        #find targets actually used\n",
    "        for tgtcol in tgtcols:\n",
    "            tgtname = str(sp.loc[sample,tgtcol]).strip().replace('-','')\n",
    "\n",
    "            if tgtname in list(targets.Target):\n",
    "                #get target info\n",
    "                tidf = tgtinfo[tgtinfo.ParamName.str[0:8]==tgtcol]\n",
    "                for tic in tidf.ParamName:\n",
    "                    disp = np.asscalar(tidf[tidf['ParamName']==tic].DisplayName)\n",
    "                    colname = 'PREPARATION STEP DETAIL: ' + tgtname + ' ' + disp\n",
    "                    val = str(sp.loc[sample,tic]).strip().replace('-','0')\n",
    "                    if disp == 'Pulses/Cycle' and val.find('/') >= 0: #if multiple values for single target, take average (should only apply to BZY82)\n",
    "                        vals = [float(v) for v in val.split('/')]\n",
    "                        val = np.mean(vals)\n",
    "                    sp.loc[sample,colname] = val\n",
    "            elif len(tgtname) > 0:\n",
    "                #write to err\n",
    "                errtxt += 'Unknown target: ' + tgtname + '\\n'\n",
    "            \n",
    "            sp = sp.drop(tgtcol, axis=1)\n",
    "        \n",
    "        for ti in list(tgtinfo.ParamName):\n",
    "            sp = sp.drop(ti, axis=1)\n",
    "\n",
    "        all_var = all_var.join(sp,on='IDENTIFIER: Sample number')\n",
    "    else:\n",
    "        log.write('*Warning: could not locate PLD parameters\\n')\n",
    "        errtxt += 'Missing PLD parameters\\n'\n",
    "        \n",
    "    all_var.to_csv(csvname,index=False)\n",
    "    log.write('Wrote ' + csvname + '\\n')\n",
    "    \n",
    "    #############\n",
    "    \"write files\"\n",
    "    #############\n",
    "    #handle errors - these files should be sent to errdir rather than procdir, and err file should be written\n",
    "    if len(errtxt) > 0:\n",
    "        #write err file\n",
    "        errname = lib + '_gen_csv.err' #os.path.join(errdir, lib + '_gen_csv.err')\n",
    "        err = open(errname,'w')\n",
    "        err.write(errtxt)\n",
    "        err.close()\n",
    "        \n",
    "        #add to errsummary\n",
    "        errsumtxt += lib + ': ' + errtxt\n",
    "        \n",
    "        #make a note in log\n",
    "        log.write('Error(s) detected, moving to ' + errdir)\n",
    "        log.close()\n",
    "        \n",
    "        #move to errdir\n",
    "        os.chdir('..')\n",
    "        os.rename(os.path.join(indir,lib), os.path.join(errdir,lib))\n",
    "        \n",
    "        #increment nerr\n",
    "        nerr += 1\n",
    "    #if no errors - move to procdir and copy outputs to outdir\n",
    "    else:\n",
    "        #move processed library to procdir\n",
    "        log.write('Finished processing library ' + lib)\n",
    "        log.close()\n",
    "\n",
    "        #copy csv and log to outdir\n",
    "        copy(csvname, os.path.join(outdir,csvname))\n",
    "        copy(logname, os.path.join(outdir,logname))\n",
    "        os.chdir('..')\n",
    "        os.rename(os.path.join(indir,lib), os.path.join(procdir,lib))\n",
    "        \n",
    "        #increment nsucc\n",
    "        nsucc += 1\n",
    "    \n",
    "if nerr > 0:\n",
    "    errsum = open(errsumname,'w')\n",
    "    errsum.write(errsumtxt)\n",
    "    errsum.close()\n",
    "\n",
    "print('Processed ' + str(nsucc) + ' sample(s) successfully')\n",
    "print('Processed ' + str(nerr) + ' sample(s) with errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Column': 'IDENTIFIER: Column',\n",
       " 'FWHM_110': 'PROPERTY: FWHM of 110 peak ($\\\\degree$)',\n",
       " 'FWHM_111': 'PROPERTY: FWHM of 111 peak ($\\\\degree$)',\n",
       " 'FWHM_200': 'PROPERTY: FWHM of 200 peak ($\\\\degree$)',\n",
       " 'MaxInt_110': 'PROPERTY: Max intensity of 110 peak (counts)',\n",
       " 'MaxInt_111': 'PROPERTY: Max intensity of 111 peak (counts)',\n",
       " 'MaxInt_200': 'PROPERTY: Max intensity of 200 peak (counts)',\n",
       " 'PeakArea_110': 'PROPERTY: Area of 110 peak',\n",
       " 'Point': 'IDENTIFIER: Point',\n",
       " 'Pos_110': 'PROPERTY: Position of 110 peak ($\\\\degree$)',\n",
       " 'Pos_111': 'PROPERTY: Position of 111 peak ($\\\\degree$)',\n",
       " 'Pos_200': 'PROPERTY: Position of 200 peak ($\\\\degree$)',\n",
       " 'Ratio_111_100': 'PROPERTY: Max intensity ratio 111:100 ',\n",
       " 'Ratio_200_100': 'PROPERTY: Max intensity ratio 200:100 ',\n",
       " 'Row': 'IDENTIFIER: Row',\n",
       " 'a_Ang': 'PROPERTY: Cubic lattice constant ($\\\\r{A}$)',\n",
       " 'd_um': 'PROPERTY: Film thickness ($\\\\mu$m)',\n",
       " 'x_mm': 'IDENTIFIER: X position (mm)',\n",
       " 'y_mm': 'IDENTIFIER: Y position (mm)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdvardict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Point',\n",
       " 'Row',\n",
       " 'Column',\n",
       " 'a_Ang',\n",
       " 'Ba_at',\n",
       " 'Column',\n",
       " 'Co_at',\n",
       " 'd_um',\n",
       " 'Fe_at',\n",
       " 'Frac_110',\n",
       " 'Frac_111',\n",
       " 'Frac_200',\n",
       " 'FWHM_110',\n",
       " 'FWHM_111',\n",
       " 'FWHM_200',\n",
       " 'MaxInt_110',\n",
       " 'MaxInt_111',\n",
       " 'MaxInt_200',\n",
       " 'Point',\n",
       " 'Pos_110',\n",
       " 'Pos_111',\n",
       " 'Pos_200',\n",
       " 'Row',\n",
       " 'x_mm',\n",
       " 'Y_at',\n",
       " 'y_mm',\n",
       " 'Zr_at']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
