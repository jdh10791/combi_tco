{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 sample(s) successfully\n",
      "Processed 1 sample(s) with errors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "\n",
    "datadir = os.path.join(os.environ['USERPROFILE'],'OneDrive - Colorado School of Mines/Research/MIDDMI/TCO/Data')\n",
    "indir = os.path.join(datadir,'in') #libraries to be processed\n",
    "outdir = os.path.join(datadir,'out') #output directory for csvs and logs\n",
    "procdir = os.path.join(datadir,'processed') #destination dir for successfully processed libraries\n",
    "errdir = os.path.join(datadir,'error') #destination dir for libraries with errors\n",
    "config = os.path.join(datadir,'config') #config/parameter file location\n",
    "\n",
    "############################\n",
    "\"read config files\"\n",
    "############################\n",
    "os.chdir(config)\n",
    "\n",
    "#load variable definitions\n",
    "#----------------------------\n",
    "vardictdf = pd.read_excel('VariableDict.xlsx')\n",
    "\n",
    "#put standard variables into dict\n",
    "stdvardf = vardictdf[vardictdf.Keyword.str[0] != '!']\n",
    "stdvars = list(stdvardf.VarName)\n",
    "headers = list(stdvardf.Keyword + ': ' + stdvardf.DisplayName)\n",
    "stdvardict = dict(zip(stdvars,headers))\n",
    "\n",
    "#composition variables\n",
    "compvardf = vardictdf[vardictdf.Keyword == '!composition']\n",
    "compvars = list(compvardf.VarName)\n",
    "\n",
    "#load PLD parameters\n",
    "#----------------------------\n",
    "prepstep = 'Pulsed Laser Deposition'\n",
    "params = pd.read_excel('PLD_params.xlsx',skiprows=3,index_col=0)\n",
    "paramdictdf = pd.read_excel('PLD_ParamDict.xlsx')\n",
    "#put standard params into dict\n",
    "stdparamdf = paramdictdf[paramdictdf.Keyword.str[0] != '!']\n",
    "stdparams = list(stdparamdf.ParamName)\n",
    "parheaders = list(stdparamdf.Keyword + ': ' + stdparamdf.DisplayName)\n",
    "stdparamdict = dict(zip(stdparams,parheaders))\n",
    "#get lists of targets & target info params\n",
    "tgtcols = list(paramdictdf[paramdictdf.Keyword == '!target'].ParamName)\n",
    "tgtinf = list(paramdictdf[paramdictdf.Keyword == '!targetinfo'].ParamName)\n",
    "#rename headers to ingestable format using paramdict\n",
    "params = params.rename(index=str, columns = stdparamdict) \n",
    "params.insert(1,'PREPARATION STEP NAME',prepstep)\n",
    "#end config\n",
    "\n",
    "#library prefix to strip out\n",
    "libprefix = 'PDAC_COM3_'\n",
    "\n",
    "########################################\n",
    "\"loop through all libraries in indir\"\n",
    "########################################\n",
    "os.chdir(indir)\n",
    "nsucc = 0\n",
    "nerr = 0\n",
    "errsumname = os.path.join(errdir, 'gen_csv_' + datetime.now().strftime('%Y%m%d_%H%M%S') + '.err')\n",
    "errsum = open(errsumname,'w')\n",
    "\n",
    "for lib in next(os.walk(indir))[1]:\n",
    "    os.chdir(lib)\n",
    "    \n",
    "    #log file\n",
    "    logname = lib + '_gen_csv.log'\n",
    "    log = open(logname, 'w')\n",
    "    log.write('Processing library ' + lib + '\\n')\n",
    "    \n",
    "    #error file\n",
    "    errtxt = ''\n",
    "    \n",
    "    #csv\n",
    "    csvname = lib + '_AllVar.csv'\n",
    "    \n",
    "    all_var = pd.DataFrame([])\n",
    "    \n",
    "    #read the Points file to determine file length\n",
    "    pts = pd.read_csv(lib + '_Points.txt',sep='\\t',usecols = [0]) \n",
    "    \n",
    "    #dataframes to store composition data\n",
    "    chm = pd.DataFrame()#np.zeros((len(pts),1),dtype='str'),columns=['formula'])\n",
    "    sitesum = pd.DataFrame(np.zeros((len(pts),2)), columns = ['A','B'])\n",
    "    \n",
    "    #track found variables\n",
    "    foundvars = ['Point','Row','Column']\n",
    "    \n",
    "    for fname in glob.glob(lib + '*.txt'):\n",
    "        vname = fname[len(lib)+1:fname.find('.txt')]\n",
    "        #regular variables\n",
    "        if vname in stdvars:\n",
    "            if len(all_var) == 0:\n",
    "                cols = None\n",
    "            else:\n",
    "                cols = [3]\n",
    "            df = pd.read_csv(fname,sep='\\t',usecols = cols)\n",
    "            all_var = pd.concat([all_var,df], axis=1)\n",
    "        #composition variables\n",
    "        elif vname in compvars:\n",
    "            dfc = pd.read_csv(fname,sep='\\t',usecols = [3])\n",
    "            elmnt = vname[:vname.find('_at')]\n",
    "            chm[elmnt] = dfc[vname] \n",
    "            sitesum.A = sitesum.A + dfc[vname]\n",
    "        \n",
    "        foundvars.append(vname)\n",
    "    \n",
    "    #identify ignored and missing variables\n",
    "    ignoredvars = np.setdiff1d(foundvars,stdvars+compvars, assume_unique=True)\n",
    "    missingstd = np.setdiff1d(stdvars,foundvars, assume_unique=True)\n",
    "    missingcomp = np.setdiff1d(compvars,foundvars, assume_unique=True)\n",
    "    if len(ignoredvars) > 0:\n",
    "        log.write('Ignored variables:\\n\\t' + '\\n\\t'.join(ignoredvars) + '\\n')\n",
    "    if len(missingstd) > 0:\n",
    "        log.write('*Warning: missing standard variables:\\n\\t' + '\\n\\t'.join(missingstd) + '\\n')\n",
    "    if len(missingcomp) > 0:\n",
    "        log.write('*Warning: missing composition variables:\\n\\t' + '\\n\\t'.join(missingcomp) + '\\n')\n",
    "        errtxt += 'Missing composition variables: ' + ', '.join(missingcomp) + '\\n'\n",
    "        \n",
    "    #determine composition\n",
    "    sitesum.B = 1 - sitesum.A\n",
    "    sitemax = sitesum.max(axis=1)\n",
    "    sitenorm = sitesum.divide(sitemax,axis=0) #normalize for the higher occupancy site\n",
    "    chmnorm = chm.divide(sitemax,axis=0)\n",
    "    chmnorm.insert(0,'Ba',list(sitenorm.B))\n",
    "    chmnorm = chmnorm.round(5)\n",
    "    for elmnt in list(chmnorm.columns):\n",
    "        chmnorm[elmnt] = elmnt + chmnorm[elmnt].map(str) \n",
    "    chmnorm['formula'] = chmnorm.apply(lambda x: ''.join(x),axis=1)\n",
    "    chmnorm['formula'] = chmnorm['formula'] + 'O3'\n",
    "\n",
    "    sample = lib[len(libprefix):]\n",
    "\n",
    "    all_var = all_var.rename(index=str, columns = stdvardict) #rename headers to ingestable format using vardict\n",
    "    all_var.insert(0,'IDENTIFIER: Sample number',sample) #add sample column        \n",
    "    all_var.insert(0,'FORMULA',list(chmnorm['formula'])) #add formula column\n",
    "    \n",
    "    #add PLD parameters corresponding to sample number\n",
    "    if sample in list(params.index) and not(pd.isnull(params.loc[sample,'IDENTIFIER: Date'])):\n",
    "        #identify targets used and reformat accordingly\n",
    "        sp = params.loc[sample,:]\n",
    "        for tgti in tgtinf:\n",
    "            tgt = tgti[0:8]\n",
    "            disp = np.asscalar(paramdictdf[paramdictdf['ParamName']==tgti].DisplayName)\n",
    "            if str(sp[tgti]).strip() in ['-','']:\n",
    "                sp = sp.drop(tgti)\n",
    "            else:\n",
    "                sp = sp.rename({tgti: 'PREPARATION STEP DETAIL: ' + sp[tgt] + ' ' + disp})\n",
    "        for tgt in tgtcols:\n",
    "            sp = sp.drop(tgt)\n",
    "\n",
    "        all_var = all_var.join(sp.to_frame().T,on='IDENTIFIER: Sample number')\n",
    "    else:\n",
    "        log.write('*Warning: could not locate PLD parameters\\n')\n",
    "        errtxt += 'Missing PLD parameters\\n'\n",
    "        \n",
    "    all_var.to_csv(csvname,index=False)\n",
    "    log.write('Wrote ' + csvname + '\\n')\n",
    "    \n",
    "    #handle errors - these files should be sent to errdir rather than procdir, and err file should be written\n",
    "    if len(errtxt) > 0:\n",
    "        #write err file\n",
    "        errname = lib + '_gen_csv.err' #os.path.join(errdir, lib + '_gen_csv.err')\n",
    "        err = open(errname,'w')\n",
    "        err.write(errtxt)\n",
    "        err.close()\n",
    "        \n",
    "        #write to errsummary\n",
    "        errsum.write(lib + ': ' + errtxt)\n",
    "        \n",
    "        #make a note in log\n",
    "        log.write('Error(s) detected, moving to ' + errdir)\n",
    "        log.close()\n",
    "        \n",
    "        #move to errdir\n",
    "        os.chdir('..')\n",
    "        os.rename(os.path.join(indir,lib), os.path.join(errdir,lib))\n",
    "        \n",
    "        #increment nerr\n",
    "        nerr += 1\n",
    "    #if no errors - move to procdir and copy outputs to outdir\n",
    "    else:\n",
    "        #move processed library to procdir\n",
    "        log.write('Finished processing library ' + lib)\n",
    "        log.close()\n",
    "\n",
    "        #copy csv and log to outdir\n",
    "        copyfile(csvname, os.path.join(outdir,csvname))\n",
    "        copyfile(logname, os.path.join(outdir,logname))\n",
    "        os.chdir('..')\n",
    "        os.rename(os.path.join(indir,lib), os.path.join(procdir,lib))\n",
    "        \n",
    "        #increment nsucc\n",
    "        nsucc += 1\n",
    "\n",
    "\n",
    "errsum.close()\n",
    "\n",
    "print('Processed ' + str(nsucc) + ' sample(s) successfully')\n",
    "print('Processed ' + str(nerr) + ' sample(s) with errors')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
